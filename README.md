# AI Avatar for Learning ğŸ“

An interactive 3D AI avatar learning assistant powered by OpenAI's GPT-4 and Text-to-Speech, featuring real-time conversation streaming and synchronized lip movements.

<div align="center">
  <img src="public/avatar.png" alt="LinguaAI Avatar Interface" width="800"/>
  <p><em>Interactive voice-to-voice conversation with a 3D AI avatar</em></p>
</div>

## âœ¨ Features

- **ğŸ¤ Speech-to-Speech Conversation**: Talk naturally with the avatar using voice input and hear real-time responses
- **ğŸ­ 3D Interactive Avatar**: Rendered using Three.js and React Three Fiber with a Ready Player Me character model
- **âš¡ Real-time Streaming**: GPT-4 responses stream in real-time with sentence-by-sentence text-to-speech generation
- **ğŸ‘„ Lip Sync Animation**: Audio-reactive lip sync using Web Audio API frequency analysis
- **ğŸ—£ï¸ Natural Speech**: OpenAI Whisper (STT) and TTS integration for realistic voice interaction
- **ğŸ’¬ Dual Input Modes**: Type or speak - both work seamlessly
- **ğŸ¨ Modern UI**: Beautiful, responsive chat interface built with Tailwind CSS
- **ğŸ”Š Seamless Audio Queue**: Multiple audio chunks play sequentially without gaps

## ğŸ› ï¸ Tech Stack

- **Framework**: [Next.js 16](https://nextjs.org/) (App Router)
- **Language**: TypeScript
- **3D Rendering**: [Three.js](https://threejs.org/), [@react-three/fiber](https://docs.pmnd.rs/react-three-fiber), [@react-three/drei](https://github.com/pmndrs/drei)
- **AI**: [OpenAI API](https://platform.openai.com/) (GPT-4o, Whisper, TTS-1)
- **Audio**: Web Audio API, MediaRecorder API
- **Styling**: [Tailwind CSS](https://tailwindcss.com/)
- **Icons**: [Lucide React](https://lucide.dev/)

## ğŸ“‹ Prerequisites

- Node.js 18+ 
- npm, yarn, pnpm, or bun
- OpenAI API Key

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone <repository-url>
cd ai-avatar-for-learning
```

### 2. Install dependencies

```bash
npm install
# or
yarn install
# or
pnpm install
```

### 3. Set up environment variables

Create a `.env.local` file in the root directory:

```bash
cp env.example .env.local
```

Edit `.env.local` and add your OpenAI API key:

```env
OPENAI_API_KEY=sk-your-api-key-here
```

### 4. Run the development server

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

### 5. Open the application

Navigate to [http://localhost:3000](http://localhost:3000) in your browser.

## ğŸ® Usage

### Voice Conversation (Recommended)

1. Click the **ğŸ¤ blue microphone button**
2. Grant microphone permission when prompted
3. Speak your message clearly
4. Click the button again to stop recording
5. Watch as your speech is transcribed and the avatar responds with voice!

### Text Conversation

1. Type your message in the input field
2. Press **Enter** or click the **pink send button**
3. The avatar will respond with voice and text

### Tips for Best Experience

- ğŸ§ Use headphones to prevent echo
- ğŸ—£ï¸ Speak clearly and at a normal pace
- ğŸ“ Allow microphone access when prompted
- ğŸ”Š Ensure your volume is up to hear the avatar
- ğŸ’¬ Try both voice and text - they work seamlessly together!

## ğŸ“ Project Structure

```
ai-avatar-for-learning/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”‚   â””â”€â”€ route.ts          # Streaming GPT-4 chat endpoint
â”‚   â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â”‚   â””â”€â”€ route.ts          # Text-to-speech endpoint
â”‚   â”‚   â””â”€â”€ stt/
â”‚   â”‚       â””â”€â”€ route.ts          # Speech-to-text endpoint (Whisper)
â”‚   â”œâ”€â”€ globals.css               # Global styles
â”‚   â”œâ”€â”€ layout.tsx                # Root layout
â”‚   â””â”€â”€ page.tsx                  # Home page
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Avatar.tsx                # 3D avatar with lip sync
â”‚   â”œâ”€â”€ Experience.tsx            # Three.js scene wrapper
â”‚   â”œâ”€â”€ Header.tsx                # App header
â”‚   â””â”€â”€ UI.tsx                    # Chat interface with voice input
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useChat.ts                # Chat logic with streaming and TTS queue
â”‚   â””â”€â”€ useAudioRecording.ts     # Audio recording with MediaRecorder
â”œâ”€â”€ public/
â”‚   â””â”€â”€ avatar.png                # App screenshot
â””â”€â”€ README.md
```

## ğŸ¯ How It Works

### Voice Input Flow (Speech-to-Speech)

1. **ğŸ¤ Voice Recording**: User clicks microphone button and speaks
2. **ğŸ”´ Recording**: Button turns red and pulses while recording
3. **âœ‹ Stop Recording**: User clicks again to stop
4. **ğŸ“ Transcription**: Audio sent to `/api/stt` using Whisper API
5. **ğŸ’¬ Text Display**: Transcribed text briefly appears in input
6. **ğŸ¤– AI Response**: Message sent to GPT-4 via `/api/chat`
7. **ğŸ“¡ Streaming**: Response streams word-by-word to UI
8. **ğŸ”Š Text-to-Speech**: Complete sentences converted to audio via `/api/tts`
9. **ğŸ­ Lip Sync**: Avatar's mouth moves naturally with speech

### Text Input Flow (Traditional Chat)

1. **âŒ¨ï¸ Type Message**: User types in the input field
2. **ğŸ“¤ Send**: User presses Enter or clicks send button
3. **ğŸ¤– AI Response**: Same as steps 6-9 above

### API Routes

#### `POST /api/chat`
- Accepts: `{ messages: Message[] }`
- Returns: Streaming text response
- Uses: OpenAI GPT-4o with streaming enabled

#### `POST /api/tts`
- Accepts: `{ text: string }`
- Returns: Audio MP3 buffer
- Uses: OpenAI TTS-1 model with "alloy" voice

#### `POST /api/stt`
- Accepts: FormData with audio file
- Returns: `{ text: string }`
- Uses: OpenAI Whisper-1 model for transcription

## ğŸ¨ Customization

### Change Avatar Model

Edit `components/Avatar.tsx`:

```typescript
const { scene } = useGLTF("YOUR_MODEL_URL.glb");
```

### Change AI Voice

Edit `app/api/tts/route.ts`:

```typescript
const mp3 = await openai.audio.speech.create({
    model: "tts-1",
    voice: "alloy", // Options: alloy, echo, fable, onyx, nova, shimmer
    input: text,
});
```

### Change AI Model

Edit `app/api/chat/route.ts`:

```typescript
const stream = await openai.chat.completions.create({
    model: "gpt-4o", // Change to gpt-3.5-turbo, gpt-4-turbo, etc.
    // ...
});
```

### Adjust Lip Sync Sensitivity

Edit `components/Avatar.tsx`:

```typescript
// Lower value = more sensitive
const volume = Math.min(1, average / 30); // Adjust 30 to your preference
```

## ğŸ› Debugging

Enable console logging to debug the application:

1. Open browser DevTools (F12)
2. Check Console tab for logs:
   - **Voice Input**: "Requesting microphone access...", "Recording started", "Transcription successful: ..."
   - **TTS**: "Generating speech for: ...", "âœ… Audio playing successfully"
   - **Lip Sync**: "Audio routing connected: element -> analyser -> destination"

## ğŸ”§ Common Issues

### Microphone Not Working
- **Permission Denied**: Check browser settings and allow microphone access
- **No Recording**: Ensure you're using HTTPS or localhost (required for MediaRecorder API)
- **Silent Recording**: Check your system microphone settings and select the correct input device
- **Browser Compatibility**: Use Chrome, Edge, or Firefox (Safari may have limitations)

### Voice Transcription Fails
- Verify OpenAI API key is valid and has Whisper API access
- Check that audio blob size is > 0 in console logs
- Ensure you speak for at least 1-2 seconds before stopping
- Check network tab for `/api/stt` request/response

### No Audio Playing (TTS)
- Check that your browser allows autoplay with user interaction
- Verify OpenAI API key is valid
- Check browser console for errors
- Ensure audio element is properly connected to Web Audio API
- Try turning up your device volume

### Lip Sync Not Working
- Verify audio is playing first (check console for "âœ… Audio playing successfully")
- Check that Web Audio API connection is established
- Try adjusting sensitivity in `Avatar.tsx` (lower value = more sensitive)

### Streaming Not Working
- Verify API route is returning streaming response
- Check network tab in DevTools for chunked transfer encoding
- Ensure `/api/chat` route has `stream: true` enabled

## ğŸ“ Scripts

- `npm run dev` - Start development server
- `npm run build` - Build for production
- `npm run start` - Start production server
- `npm run lint` - Run ESLint

## ğŸš¢ Deployment

### Deploy on Vercel

The easiest way to deploy is using [Vercel](https://vercel.com):

1. Push your code to GitHub
2. Import the project in Vercel
3. Add environment variables (`OPENAI_API_KEY`)
4. Deploy!

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/your-repo)

## ğŸ“„ License

MIT

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ™ Acknowledgments

- [Ready Player Me](https://readyplayer.me/) for the 3D avatar model
- [OpenAI](https://openai.com/) for GPT-4 and TTS APIs
- [Pmndrs](https://pmnd.rs/) for React Three Fiber ecosystem
